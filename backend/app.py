import os
import shutil
from datetime import date
from typing import Optional
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import fitz  # PyMuPDF
import psycopg2
import openai
import requests
from bs4 import BeautifulSoup
from fastapi import Query
from langchain.text_splitter import RecursiveCharacterTextSplitter


# ‚îÄ‚îÄ Setup upload directory ‚îÄ‚îÄ
BASE_DIR = os.path.dirname(__file__)
UPLOAD_DIR = os.path.join(BASE_DIR, "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ‚îÄ‚îÄ Load env & initialize clients ‚îÄ‚îÄ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY missing in .env")
openai.api_key = OPENAI_API_KEY

# Local embedder
model = SentenceTransformer("BAAI/bge-small-en-v1.5")

# ‚îÄ‚îÄ FastAPI setup ‚îÄ‚îÄ
app = FastAPI(title="SmartFusion RAG API")

# ‚îÄ‚îÄ CORS (allow your React dev server) ‚îÄ‚îÄ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ Database connection helper ‚îÄ‚îÄ
def get_db_connection():
    try:
        return psycopg2.connect(
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD"),
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT"),
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB connection failed: {e}")

# ‚îÄ‚îÄ PDF/Text extraction & chunking ‚îÄ‚îÄ
def extract_pdf_text(path: str) -> str:
    try:
        pages = []
        with fitz.open(path) as doc:
            for page in doc:
                # Use block layout to preserve structure
                blocks = page.get_text("blocks")  # (x0, y0, x1, y1, "text", block_no, block_type)
                blocks = sorted(blocks, key=lambda b: (b[1], b[0]))  # sort top-to-bottom, then left-to-right
                page_text = "\n".join(block[4].strip() for block in blocks if block[4].strip())
                pages.append(page_text)
        return "\n".join(pages)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PDF extraction failed: {e}")

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> list[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ".", " "],  # smart fallbacks
        keep_separator=True
    )
    return splitter.split_text(text)

# ‚îÄ‚îÄ Ingest PDF endpoint ‚îÄ‚îÄ
@app.post("/ingest_pdf")
async def ingest_pdf(
    file: UploadFile = File(...),
    country: str = Form("Unknown"),
    target_group: str = Form("Unknown"),
    owner: str = Form("Unknown"),
):
    temp_path = os.path.join(UPLOAD_DIR, file.filename)
    file.file.seek(0)
    with open(temp_path, "wb") as out:
        shutil.copyfileobj(file.file, out)

    size = os.path.getsize(temp_path)
    open_error = None
    try:
        doc = fitz.open(temp_path)
        if doc.is_encrypted and not doc.authenticate(""):
            open_error = "encrypted or password-protected"
        doc.close()
    except Exception as e:
        open_error = str(e)

    if open_error:
        return {
            "detail": "üö® PDF open failed",
            "written_bytes": size,
            "open_error": open_error
        }

    full_text = extract_pdf_text(temp_path)
    chunks = chunk_text(full_text)
    for i, ch in enumerate(chunks):
        print(f"üß© Chunk {i+1}:\n{ch[:80]}...\n")
    conn = get_db_connection()
    cur = conn.cursor()
    for chunk in chunks:
        emb = model.encode(chunk).tolist()
        cur.execute(
            """
            INSERT INTO documents
              (filename, country, target_group, owner, creation_date, full_text, content_embedding)
            VALUES (%s,%s,%s,%s,%s,%s,%s)
            """,
            (file.filename, country, target_group, owner, date.today(), chunk, emb)
        )
    conn.commit()
    cur.close()
    conn.close()

    return {
        "detail": f"Ingested {len(chunks)} chunks from {file.filename}",
        "written_bytes": size
    }

# ‚îÄ‚îÄ Ingest URL endpoint ‚îÄ‚îÄ
@app.post("/ingest_url")
async def ingest_url(
    url: str = Query(..., description="Webpage URL to ingest"),
    country: str = Query("Unknown"),
    target_group: str = Query("Unknown"),
    owner: str = Query("Unknown")
):
    resp = requests.get(url)
    if resp.status_code != 200:
        raise HTTPException(status_code=400, detail=f"Failed to fetch {url}: {resp.status_code}")
    soup = BeautifulSoup(resp.text, "html.parser")
    full_text = " ".join(soup.stripped_strings)
    chunks = chunk_text(full_text)

    conn = get_db_connection()
    cur = conn.cursor()
    for i, chunk in enumerate(chunks):
        emb = model.encode(chunk).tolist()
        cur.execute(
            """
            INSERT INTO documents
              (filename, country, target_group, owner, creation_date, full_text, content_embedding)
            VALUES (%s,%s,%s,%s,%s,%s,%s)
            """,
            (f"{url}_chunk{i}", country, target_group, owner, date.today(), chunk, emb)
        )
    conn.commit()
    cur.close()
    conn.close()

    return {"detail": f"Ingested {len(chunks)} chunks from {url}"}

# ‚îÄ‚îÄ Retrieval logic ‚îÄ‚îÄ
def retrieve(query: str, k: int = 5):
    # 1) Encode the user‚Äôs question
    q_emb = model.encode(query).tolist()

    # 2) Build a global search over all chunks
    sql = """
      SELECT filename,
             full_text,
             content_embedding <=> (%s)::vector AS distance
        FROM documents
       ORDER BY distance ASC
       LIMIT %s
    """
    params = [q_emb, k]

    # (optional) debug print
    print("üîç SQL:", sql.replace("\n", " "))
    print("üî¢ params:", params)

    conn = get_db_connection()
    cur  = conn.cursor()
    cur.execute(sql, params)
    rows = cur.fetchall()
    print(f"‚úÖ Retrieved {len(rows)} chunks across all PDFs")
    cur.close()
    conn.close()
    return rows

# ‚îÄ‚îÄ Request schema ‚îÄ‚îÄ
class QueryRequest(BaseModel):
    question: str
    top_k:    int              = 5
    filename: Optional[str]    = None

# ‚îÄ‚îÄ Query endpoint ‚îÄ‚îÄ
@app.post("/query")
def ask_question(req: QueryRequest):
    try:
        # retrieve across all PDFs
        hits = retrieve(req.question, req.top_k)
        return [
            {"filename": fn, "snippet": snip}
            for fn, snip, _ in hits
        ]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ‚îÄ‚îÄ Answer endpoint ‚îÄ‚îÄ
@app.post("/answer")
def answer_question(req: QueryRequest):
    try:
        # 1) fetch top-k chunks globally
        hits = retrieve(req.question, req.top_k)

        # 2) build the combined context
        context = "\n\n".join(chunk for _, chunk, _ in hits)

        # 3) ask GPT using only that context
        prompt = (
            "You are a helpful assistant. Use ONLY the context below to answer.\n\n"
            f"Context:\n{context}\n\n"
            f"Question: {req.question}\nAnswer:"
        )
        resp = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system",  "content": "You are a helpful assistant."},
                {"role": "user",    "content": prompt},
            ],
        )
        answer = resp.choices[0].message.content.strip()

        # 4) return the answer plus which files it came from
        sources = [
            {"filename": fn, "similarity": round(1.0/(1.0+dist), 3)}
            for fn, _, dist in hits
        ]

        return {"answer": answer, "sources": sources}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    # ‚îÄ‚îÄ List all stored filenames ‚îÄ‚îÄ
@app.get("/documents")
def list_documents():
    conn = get_db_connection()
    cur  = conn.cursor()
    cur.execute("""
      SELECT filename
        FROM documents
    GROUP BY filename
    ORDER BY MAX(creation_date) DESC
    """)
    files = [row[0] for row in cur.fetchall()]
    cur.close()
    conn.close()
    return files

# ‚îÄ‚îÄ Delete a document‚Äôs chunks by filename ‚îÄ‚îÄ
@app.delete("/documents")
def delete_document(filename: str = Query(..., description="Filename to delete")):
    conn = get_db_connection()
    cur  = conn.cursor()
    cur.execute("DELETE FROM documents WHERE filename = %s;", (filename,))
    conn.commit()
    cur.close()
    conn.close()
    return {"detail": f"Deleted all chunks for {filename}"}

    
# ‚îÄ‚îÄ Startup log ‚îÄ‚îÄ
@app.on_event("startup")
def on_startup():
    print("üîå SmartFusion RAG API starting‚Ä¶")
    print(f"‚úîÔ∏è  OpenAI key loaded: {'yes' if openai.api_key else 'no'}")
    print("üöÄ  Endpoints: POST /ingest_pdf, /ingest_url, /query, /answer")
